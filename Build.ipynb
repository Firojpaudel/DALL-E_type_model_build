{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## Building a Dall-e type model\n\n---","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"code","source":"## pip installs \n! pip install transformers torch torchvision ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"#### 1. Data Preparation\n\n---","metadata":{}},{"cell_type":"markdown","source":"Will be using the COCO dataset. ","metadata":{}},{"cell_type":"code","source":"#@ All imports here\n\nfrom datasets import load_dataset\nfrom torchvision import transforms\nfrom PIL import Image\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T12:45:00.284195Z","iopub.execute_input":"2025-01-24T12:45:00.284573Z","iopub.status.idle":"2025-01-24T12:45:09.080831Z","shell.execute_reply.started":"2025-01-24T12:45:00.284543Z","shell.execute_reply":"2025-01-24T12:45:09.079773Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"dataset= load_dataset(\"HuggingFaceM4/COCO\", split=\"train[:1%]\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T12:45:11.162049Z","iopub.execute_input":"2025-01-24T12:45:11.162616Z","iopub.status.idle":"2025-01-24T12:54:38.631352Z","shell.execute_reply.started":"2025-01-24T12:45:11.162571Z","shell.execute_reply":"2025-01-24T12:54:38.630222Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/3.66k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d8ace4b7e69447eeacd930a3d3e40f9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"COCO.py:   0%|          | 0.00/9.47k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d890615948944628cd50a79ed2e959b"}},"metadata":{}},{"output_type":"stream","name":"stdin","text":"The repository for HuggingFaceM4/COCO contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/HuggingFaceM4/COCO.\nYou can avoid this prompt in future by passing the argument `trust_remote_code=True`.\n\nDo you wish to run the custom code? [y/N]  y\n"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/36.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"56180fef68684debbf066df6ded8e907"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/13.5G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ffe36b5fafb491793fa6632199c3cc2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/6.65G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9b1139b381bc41c28c9afc1569308925"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1991e74fe40844f1966ba509b3964e82"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a9bf737597cc47cd8bb0e9e407450d05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"77685ce5419a4ef7a4fca8b4c12300bf"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"dataset[0]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:53:30.805965Z","iopub.execute_input":"2025-01-24T13:53:30.806341Z","iopub.status.idle":"2025-01-24T13:53:30.818963Z","shell.execute_reply.started":"2025-01-24T13:53:30.806312Z","shell.execute_reply":"2025-01-24T13:53:30.817793Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x480>,\n 'filepath': 'COCO_val2014_000000522418.jpg',\n 'sentids': [681330, 686718, 688839, 693159, 693204],\n 'filename': 'COCO_val2014_000000522418.jpg',\n 'imgid': 1,\n 'split': 'restval',\n 'sentences': {'tokens': ['a',\n   'woman',\n   'wearing',\n   'a',\n   'net',\n   'on',\n   'her',\n   'head',\n   'cutting',\n   'a',\n   'cake'],\n  'raw': 'A woman wearing a net on her head cutting a cake. ',\n  'imgid': 1,\n  'sentid': 681330},\n 'cocoid': 522418}"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"##@ Checkng the dataset\n\nsample = dataset[0]\nimage_path= sample['filepath']\nimage_name= sample['filename']\ncaption = sample['sentences']['raw']\n\nimage = Image.open(image_path).convert('RGB')\nplt.figure(figsize=(10,10))\nplt.imshow(image)\nplt.axis('off')\nplt.title(f\"Caption: {caption}\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-01-24T13:54:37.396863Z","iopub.execute_input":"2025-01-24T13:54:37.397247Z","iopub.status.idle":"2025-01-24T13:54:37.425927Z","shell.execute_reply.started":"2025-01-24T13:54:37.397215Z","shell.execute_reply":"2025-01-24T13:54:37.424528Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-20-8329191f37f3>\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mcaption\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sentences'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'raw'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_path\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode, formats)\u001b[0m\n\u001b[1;32m   3467\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3468\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3469\u001b[0;31m         \u001b[0mfp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuiltins\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3470\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3471\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/kaggle/working/COCO_val2014_000000522418.jpg'"],"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: '/kaggle/working/COCO_val2014_000000522418.jpg'","output_type":"error"}],"execution_count":20},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}